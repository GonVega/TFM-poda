# Documentaci√≥n: M√©todos de Optimizaci√≥n de Post-Poda

## √çndice
1. [Introducci√≥n](#introducci√≥n)
2. [Post-Poda Cl√°sica](#post-poda-cl√°sica)
3. [Poda Greedy para Velocidad](#poda-greedy-para-velocidad)
4. [Comparaci√≥n de M√©todos](#comparaci√≥n-de-m√©todos)
5. [Exportaci√≥n a If-Else](#exportaci√≥n-a-if-else)
6. [Ejemplos de Uso](#ejemplos-de-uso)
7. [Recomendaciones](#recomendaciones)

---

## Introducci√≥n

Este proyecto implementa dos m√©todos de optimizaci√≥n de post-poda para √°rboles de decisi√≥n generados a partir de redes neuronales. Los m√©todos permiten reducir la complejidad del √°rbol manteniendo la precisi√≥n del modelo.

### Objetivos de la Poda
- **Reducir el n√∫mero de nodos** del √°rbol
- **Mantener la precisi√≥n** del modelo
- **Acelerar la inferencia** 
- **Mejorar la interpretabilidad**

---

## Post-Poda Cl√°sica

### Descripci√≥n
La post-poda cl√°sica recorre el √°rbol en orden postorden y eval√∫a si cada nodo puede ser convertido en hoja sin degradar significativamente el rendimiento del modelo.

### Algoritmo
1. **Recorrido postorden**: Se visita cada nodo despu√©s de sus hijos
2. **Evaluaci√≥n de poda**: Para cada nodo:
   - Se convierte temporalmente en hoja
   - Se asigna la clase mayoritaria de las muestras que llegan al nodo
   - Se eval√∫a el rendimiento en el conjunto de validaci√≥n
3. **Decisi√≥n de poda**: 
   - Si el rendimiento no empeora ‚Üí se mantiene la poda
   - Si el rendimiento empeora ‚Üí se restaura el nodo original

### Implementaci√≥n
```python
from nndt_lib.post_pruning import post_prune_tree

# Aplicar post-poda cl√°sica
post_prune_tree(tree, X_val, y_val, task='classification', metric='accuracy')
```

### Par√°metros
- `tree`: √Årbol de decisi√≥n a podar
- `X_val, y_val`: Conjunto de validaci√≥n
- `task`: 'classification' o 'regression'
- `metric`: M√©trica a optimizar ('accuracy', 'mean_squared_error', etc.)

### Ventajas
- ‚úÖ Conservadora: mantiene la precisi√≥n
- ‚úÖ Robusta: funciona bien en diferentes datasets
- ‚úÖ Interpretable: f√°cil de entender

### Desventajas
- ‚ùå Puede no reducir mucho el √°rbol si el modelo est√° bien ajustado
- ‚ùå Depende de la calidad del conjunto de validaci√≥n

---

## Poda Greedy para Velocidad

### Descripci√≥n
La poda greedy es m√°s agresiva que la cl√°sica. Permite una peque√±a p√©rdida de precisi√≥n (controlada por tolerancia) a cambio de una mayor reducci√≥n del √°rbol.

### Algoritmo
1. **Recorrido postorden**: Similar a la post-poda cl√°sica
2. **Evaluaci√≥n con tolerancia**: Para cada nodo:
   - Se convierte temporalmente en hoja
   - Se eval√∫a el rendimiento
   - Se compara con el rendimiento original
3. **Decisi√≥n de poda**:
   - Si la p√©rdida ‚â§ tolerancia ‚Üí se acepta la poda
   - Si la p√©rdida > tolerancia ‚Üí se rechaza la poda

### Implementaci√≥n
```python
from nndt_lib.prune_for_speed import prune_tree_for_speed

# Aplicar poda greedy con tolerancia baja
prune_tree_for_speed(tree, X_val, y_val, 
                    task='classification', 
                    metric='accuracy', 
                    tolerance=0.00005)
```

### Par√°metros
- `tree`: √Årbol de decisi√≥n a podar
- `X_val, y_val`: Conjunto de validaci√≥n
- `task`: 'classification' o 'regression'
- `metric`: M√©trica a optimizar
- `tolerance`: Tolerancia m√°xima de p√©rdida de rendimiento

### Ventajas
- ‚úÖ M√°s agresiva: reduce m√°s el √°rbol
- ‚úÖ Controlable: par√°metro de tolerancia ajustable
- ‚úÖ Eficiente: mayor reducci√≥n de tiempo de inferencia

### Desventajas
- ‚ùå Puede degradar la precisi√≥n si la tolerancia es muy alta
- ‚ùå Requiere ajuste del par√°metro de tolerancia
- ‚ùå Menos robusta que la post-poda cl√°sica

---

## Comparaci√≥n de M√©todos

| Aspecto | Post-Poda Cl√°sica | Poda Greedy |
|---------|-------------------|-------------|
| **Conservadurismo** | Alta | Baja |
| **Reducci√≥n de nodos** | Moderada | Alta |
| **Mantenimiento de precisi√≥n** | Excelente | Variable |
| **Facilidad de uso** | Alta | Media |
| **Ajuste de par√°metros** | M√≠nimo | Requerido |
| **Robustez** | Alta | Media |

### Ejemplo de Resultados T√≠picos

```
√Årbol Original:
- Nodos: 1023
- Accuracy: 0.7482
- Tiempo: 1.14s

Post-Poda Cl√°sica:
- Nodos: 229
- Accuracy: 0.7514
- Tiempo: 1.14s

Poda Greedy (tolerance=0.00005):
- Nodos: 171
- Accuracy: 0.7418
- Tiempo: 1.14s
```

---

## Exportaci√≥n a If-Else

### Descripci√≥n
Para m√°xima eficiencia en inferencia, el √°rbol podado puede exportarse a una funci√≥n Python en formato if-else puro, eliminando la sobrecarga de objetos.

### Implementaci√≥n
```python
from nndt_lib.tree_to_ifelse import export_tree_to_ifelse

# Exportar √°rbol a funci√≥n if-else
code = export_tree_to_ifelse(tree)
with open('tree_ifelse_exported.py', 'w') as f:
    f.write(code)

# Usar la funci√≥n generada
local_vars = {}
exec(code, globals(), local_vars)
tree_predict = local_vars['tree_predict']

# Inferencia ultrarr√°pida
predictions = [tree_predict(x) for x in X_test]
```

### Ventajas de la Exportaci√≥n
- ‚ö° **Inferencia ultrarr√°pida**: Sin sobrecarga de objetos
- üì¶ **Portabilidad**: C√≥digo Python puro
- üîß **Simplicidad**: F√°cil de integrar en otros sistemas
- üìä **Consistencia**: Reproduce exactamente el comportamiento del √°rbol

---

## Ejemplos de Uso

### Ejemplo 1: Post-Poda Cl√°sica
```python
from nndt_lib.binary_tree import Tree
from nndt_lib.post_pruning import post_prune_tree

# Crear √°rbol desde red neuronal
tree = Tree(model)
tree.create_DT(auto_prune=True, verbose=False)

# Aplicar post-poda cl√°sica
post_prune_tree(tree, X_val, y_val, task='classification', metric='accuracy')

# Evaluar resultados
acc = tree.evaluate_model(X_test, y_test, task='classification', metrics=['accuracy'])
print(f"Accuracy tras post-poda: {acc['accuracy']:.4f}")
```

### Ejemplo 2: Poda Greedy
```python
from nndt_lib.prune_for_speed import prune_tree_for_speed

# Aplicar poda greedy con tolerancia baja
prune_tree_for_speed(tree, X_val, y_val, 
                    task='classification', 
                    metric='accuracy', 
                    tolerance=0.00005)

# Evaluar resultados
acc = tree.evaluate_model(X_test, y_test, task='classification', metrics=['accuracy'])
print(f"Accuracy tras poda greedy: {acc['accuracy']:.4f}")
```

### Ejemplo 3: Combinaci√≥n de M√©todos
```python
# 1. Post-poda cl√°sica
post_prune_tree(tree, X_val, y_val, task='classification', metric='accuracy')

# 2. Poda greedy (opcional)
prune_tree_for_speed(tree, X_val, y_val, 
                    task='classification', 
                    metric='accuracy', 
                    tolerance=0.00005)

# 3. Exportar a if-else
code = export_tree_to_ifelse(tree)
with open('final_tree.py', 'w') as f:
    f.write(code)
```

---

## Recomendaciones

### Cu√°ndo Usar Post-Poda Cl√°sica
- ‚úÖ Cuando la precisi√≥n es cr√≠tica
- ‚úÖ En producci√≥n donde la robustez es importante
- ‚úÖ Cuando no se requiere m√°xima reducci√≥n del √°rbol
- ‚úÖ Para datasets peque√±os o medianos

### Cu√°ndo Usar Poda Greedy
- ‚úÖ Cuando se necesita m√°xima reducci√≥n del √°rbol
- ‚úÖ Cuando se puede tolerar una peque√±a p√©rdida de precisi√≥n
- ‚úÖ Para sistemas con restricciones de memoria/tiempo
- ‚úÖ Cuando se tiene un buen conjunto de validaci√≥n

### Ajuste de Par√°metros

#### Tolerancia para Poda Greedy
```python
# Muy conservadora (mantiene precisi√≥n)
tolerance = 0.00001

# Moderada (balance entre precisi√≥n y reducci√≥n)
tolerance = 0.00005

# Agresiva (m√°xima reducci√≥n)
tolerance = 0.001
```

### Flujo Recomendado
1. **Entrenar red neuronal** y convertir a √°rbol
2. **Aplicar post-poda cl√°sica** como base
3. **Evaluar rendimiento** en conjunto de validaci√≥n
4. **Opcionalmente aplicar poda greedy** con tolerancia baja
5. **Exportar a if-else** para inferencia ultrarr√°pida

### Monitoreo
- üìä Comparar accuracy antes y despu√©s de cada poda
- ‚è±Ô∏è Medir tiempo de inferencia
- üìà Contar n√∫mero de nodos y profundidad media
- üîç Verificar que las predicciones sean consistentes

---

## Conclusi√≥n

Los dos m√©todos de post-poda ofrecen diferentes estrategias de optimizaci√≥n:

- **Post-poda cl√°sica**: Conservadora, robusta, ideal para producci√≥n
- **Poda greedy**: Agresiva, eficiente, ideal para sistemas con restricciones

La combinaci√≥n de ambos m√©todos, seguida de la exportaci√≥n a if-else, proporciona la m√°xima optimizaci√≥n manteniendo la precisi√≥n del modelo.

La elecci√≥n entre m√©todos depender√° de los requisitos espec√≠ficos de cada aplicaci√≥n: precisi√≥n vs. eficiencia, robustez vs. simplicidad.

---

## Anexo: Descripci√≥n Acad√©mica para TFM

### Post-Poda Cl√°sica

La post-poda cl√°sica implementa un algoritmo de optimizaci√≥n basado en el principio de minimizaci√≥n del error de generalizaci√≥n. Este m√©todo recorre el √°rbol de decisi√≥n en orden postorden, evaluando cada nodo interno para determinar si puede ser convertido en un nodo hoja sin degradar significativamente el rendimiento del modelo. El proceso se fundamenta en la hip√≥tesis de que ciertos sub√°rboles pueden ser redundantes o contribuir al sobreajuste del modelo, especialmente cuando se han generado a partir de redes neuronales complejas.

La estrategia de evaluaci√≥n utiliza un conjunto de validaci√≥n independiente para medir el impacto de cada operaci√≥n de poda. Para cada nodo candidato, el algoritmo simula temporalmente su conversi√≥n a hoja asign√°ndole la clase mayoritaria de las muestras que llegan a ese nodo. Si la m√©trica de rendimiento (t√≠picamente accuracy para clasificaci√≥n) no se deteriora tras esta modificaci√≥n, la poda se mantiene; en caso contrario, se restaura la estructura original del nodo. Este enfoque conservador garantiza que la precisi√≥n del modelo se mantenga o mejore, aunque puede resultar en una reducci√≥n m√°s moderada de la complejidad del √°rbol.

### Poda Greedy para Velocidad

La poda greedy para velocidad representa una extensi√≥n m√°s agresiva del algoritmo de post-poda cl√°sica, introduciendo un par√°metro de tolerancia que permite un control expl√≠cito sobre el trade-off entre simplicidad del modelo y precisi√≥n. A diferencia del m√©todo conservador anterior, esta t√©cnica acepta una degradaci√≥n controlada del rendimiento a cambio de una reducci√≥n m√°s sustancial en la complejidad computacional del √°rbol. El algoritmo mantiene la estructura de recorrido postorden pero modifica el criterio de decisi√≥n para incorporar una tolerancia predefinida.

La innovaci√≥n principal de este m√©todo radica en su capacidad de balancear autom√°ticamente la precisi√≥n y la eficiencia mediante el par√°metro de tolerancia. Este valor act√∫a como un umbral que determina la m√°xima p√©rdida de rendimiento aceptable para cada operaci√≥n de poda individual. El proceso iterativo eval√∫a cada nodo candidato comparando la m√©trica de rendimiento antes y despu√©s de la simulaci√≥n de poda, aceptando la modificaci√≥n √∫nicamente si la diferencia no excede el umbral establecido. Esta aproximaci√≥n permite alcanzar reducciones significativas en el n√∫mero de nodos y la profundidad media del √°rbol, lo que se traduce en mejoras sustanciales en los tiempos de inferencia, especialmente cr√≠ticas en aplicaciones que requieren latencia m√≠nima o procesamiento en tiempo real. 