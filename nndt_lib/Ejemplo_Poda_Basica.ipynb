{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ejemplo: Poda de Árboles de Decisión\n",
    "\n",
    "Este notebook demuestra el proceso de optimización de árboles de decisión generados a partir de redes neuronales, aplicando dos métodos de post-poda:\n",
    "\n",
    "1. **Post-poda clásica**: Conservadora, mantiene la precisión\n",
    "2. **Poda greedy para velocidad**: Agresiva, permite pequeña pérdida de precisión\n",
    "\n",
    "## Objetivos\n",
    "- Reducir la complejidad del árbol\n",
    "- Mantener la precisión del modelo\n",
    "- Acelerar la inferencia\n",
    "- Mejorar la interpretabilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importaciones y Configuración"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow import keras\n",
    "import time\n",
    "from nndt_lib.binary_tree import Tree\n",
    "from nndt_lib.post_pruning import post_prune_tree\n",
    "from nndt_lib.prune_for_speed import prune_tree_for_speed\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar estilo de gráficos\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Todas las librerías importadas correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Funciones Auxiliares\n",
    "\n",
    "Definimos funciones útiles para el análisis del árbol:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    \"\"\"Cuenta el número total de nodos en el árbol\"\"\"\n",
    "    node_dict = tree.to_dict()\n",
    "    return sum(len(nodes) for nodes in node_dict.values())\n",
    "\n",
    "def check_leaf_values(node):\n",
    "    \"\"\"Verifica que los valores de las hojas sean válidos (0 o 1)\"\"\"\n",
    "    if getattr(node, '_is_leaf', False) or len(getattr(node, '_child_nodes', [])) == 0:\n",
    "        if hasattr(node, '_leaf_value'):\n",
    "            if getattr(node, '_leaf_value') not in [0, 1]:\n",
    "                print(f\"[ADVERTENCIA] Nodo hoja con _leaf_value inesperado: {getattr(node, '_leaf_value')}\")\n",
    "    for child in getattr(node, '_child_nodes', []):\n",
    "        check_leaf_values(child)\n",
    "\n",
    "def print_confusion(title, tree, X, y):\n",
    "    \"\"\"Imprime la matriz de confusión\"\"\"\n",
    "    y_pred = tree.predict(X, task='classification')\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(f\"{title}\\n{cm}\\n\")\n",
    "\n",
    "def path_length(x, tree):\n",
    "    \"\"\"Calcula la profundidad del camino recorrido para una muestra\"\"\"\n",
    "    current = tree.root\n",
    "    x_aug = np.insert(x, 0, 1)\n",
    "    depth = 0\n",
    "    while len(current.child_nodes) > 0:\n",
    "        eval_ = current.eval_node(x_aug)\n",
    "        if eval_ <= 0:\n",
    "            next_idx = 0\n",
    "        else:\n",
    "            next_idx = 1\n",
    "        if next_idx >= len(current.child_nodes):\n",
    "            break\n",
    "        current = current.child_nodes[next_idx]\n",
    "        depth += 1\n",
    "    return depth\n",
    "\n",
    "print(\"Funciones auxiliares definidas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Carga y Preparación de Datos\n",
    "\n",
    "Cargamos el dataset de diabetes y aplicamos balanceo para evitar sesgos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar dataset\n",
    "print(\"Cargando dataset de diabetes...\")\n",
    "df = pd.read_csv('cdc_diabetes_health_indicators.csv')\n",
    "\n",
    "# Limpiar datos faltantes\n",
    "if df.isnull().values.any():\n",
    "    df = df.dropna()\n",
    "    print(\"Datos faltantes eliminados\")\n",
    "\n",
    "# Balancear clases\n",
    "target_col = 'Diabetes_binary'\n",
    "class_0 = df[df[target_col] == 0]\n",
    "class_1 = df[df[target_col] == 1]\n",
    "n_min = min(len(class_0), len(class_1))\n",
    "class_0_bal = class_0.sample(n=n_min, random_state=42)\n",
    "class_1_bal = class_1.sample(n=n_min, random_state=42)\n",
    "df_bal = pd.concat([class_0_bal, class_1_bal]).sample(frac=1, random_state=42)\n",
    "\n",
    "print(f\"Dataset balanceado:\")\n",
    "print(f\"   - Clase 0: {len(class_0_bal)} muestras\")\n",
    "print(f\"   - Clase 1: {len(class_1_bal)} muestras\")\n",
    "print(f\"   - Total: {len(df_bal)} muestras\")\n",
    "\n",
    "# Preparar features y target\n",
    "X = df_bal.drop(columns=[target_col])\n",
    "y = df_bal[target_col]\n",
    "\n",
    "# Escalar datos\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Dividir en train/val/test\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X_scaled, y, test_size=0.3, random_state=42, stratify=y)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp)\n",
    "\n",
    "print(f\"Datos divididos:\")\n",
    "print(f\"   - Train: {X_train.shape[0]} muestras\")\n",
    "print(f\"   - Val: {X_val.shape[0]} muestras\")\n",
    "print(f\"   - Test: {X_test.shape[0]} muestras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Entrenamiento de Red Neuronal\n",
    "\n",
    "Creamos y entrenamos una red neuronal que servirá como base para generar el árbol de decisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Entrenando red neuronal...\")\n",
    "\n",
    "# Crear modelo\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Input(shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(5, activation='relu'),\n",
    "    keras.layers.Dense(3, activation='relu'),\n",
    "    keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compilar modelo\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Entrenar modelo\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, \n",
    "                   validation_data=(X_val, y_val), verbose=1)\n",
    "\n",
    "# Evaluar rendimiento\n",
    "val_loss, val_acc = model.evaluate(X_val, y_val, verbose=0)\n",
    "print(f\"Red neuronal entrenada:\")\n",
    "print(f\"   - Accuracy en validación: {val_acc:.4f}\")\n",
    "print(f\"   - Loss en validación: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conversión a Árbol de Decisión\n",
    "\n",
    "Convertimos la red neuronal en un árbol de decisión binario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Construyendo árbol de decisión a partir de la red neuronal...\")\n",
    "\n",
    "# Crear árbol\n",
    "tree = Tree(model)\n",
    "tree.create_DT(auto_prune=False, verbose=False)\n",
    "\n",
    "# Información del árbol original\n",
    "n_before = count_nodes(tree)\n",
    "print(f\"Árbol creado:\")\n",
    "print(f\"   - Número de nodos: {n_before}\")\n",
    "print(f\"   - Profundidad máxima: {tree.get_depth()}\")\n",
    "\n",
    "# Verificar valores de hojas\n",
    "check_leaf_values(tree.root)\n",
    "print(f\"   - Valores únicos en predicciones: {np.unique(tree.predict(X_val, task='classification'))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluación Inicial (Antes de la Poda)\n",
    "\n",
    "Evaluamos el rendimiento del árbol original:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluando árbol original...\")\n",
    "\n",
    "# Métricas de rendimiento\n",
    "acc_before = tree.evaluate_model(X_val, y_val, task='classification', metrics=['accuracy'])['accuracy']\n",
    "\n",
    "# Tiempo de inferencia\n",
    "start = time.time()\n",
    "_ = tree.predict(X_val, task='classification')\n",
    "t_before = time.time() - start\n",
    "\n",
    "# Profundidad media\n",
    "profundidad_media_before = np.mean([path_length(x, tree) for x in X_scaled])\n",
    "\n",
    "print(f\"Resultados del árbol original:\")\n",
    "print(f\"   - Accuracy: {acc_before:.4f}\")\n",
    "print(f\"   - Nodos: {n_before}\")\n",
    "print(f\"   - Tiempo de inferencia: {t_before:.4f}s\")\n",
    "print(f\"   - Profundidad media: {profundidad_media_before:.2f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "print_confusion('Matriz de confusión ANTES de la poda:', tree, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Post-Poda Clásica\n",
    "\n",
    "Aplicamos el primer método de optimización: post-poda conservadora que mantiene la precisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aplicando post-poda clásica...\")\n",
    "\n",
    "# Aplicar post-poda\n",
    "post_prune_tree(tree, X_val, y_val, task='classification', metric='accuracy')\n",
    "\n",
    "# Verificar valores de hojas\n",
    "check_leaf_values(tree.root)\n",
    "print(f\"   - Valores únicos tras post-poda: {np.unique(tree.predict(X_val, task='classification'))}\")\n",
    "\n",
    "# Evaluar resultados\n",
    "acc_post = tree.evaluate_model(X_val, y_val, task='classification', metrics=['accuracy'])['accuracy']\n",
    "n_post = count_nodes(tree)\n",
    "\n",
    "start = time.time()\n",
    "_ = tree.predict(X_val, task='classification')\n",
    "t_post = time.time() - start\n",
    "\n",
    "profundidad_media_post = np.mean([path_length(x, tree) for x in X_scaled])\n",
    "\n",
    "print(f\"Resultados tras post-poda clásica:\")\n",
    "print(f\"   - Accuracy: {acc_post:.4f}\")\n",
    "print(f\"   - Nodos: {n_post} (reducción: {n_before - n_post} nodos)\")\n",
    "print(f\"   - Tiempo de inferencia: {t_post:.4f}s\")\n",
    "print(f\"   - Profundidad media: {profundidad_media_post:.2f}\")\n",
    "print(f\"   - Cambio en accuracy: {acc_post - acc_before:+.4f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "print_confusion('Matriz de confusión DESPUÉS de post-poda:', tree, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Poda Greedy para Velocidad\n",
    "\n",
    "Aplicamos el segundo método: poda agresiva que permite pequeña pérdida de precisión:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Aplicando poda greedy para velocidad...\")\n",
    "\n",
    "# Configurar tolerancia (0.01% de pérdida máxima)\n",
    "tolerance = 0.0001\n",
    "print(f\"   - Tolerancia configurada: {tolerance:.5f}\")\n",
    "\n",
    "# Aplicar poda greedy\n",
    "prune_tree_for_speed(tree, X_val, y_val, task='classification', metric='accuracy', tolerance=tolerance)\n",
    "\n",
    "# Verificar valores de hojas\n",
    "check_leaf_values(tree.root)\n",
    "print(f\"   - Valores únicos tras greedy: {np.unique(tree.predict(X_val, task='classification'))}\")\n",
    "\n",
    "# Evaluar resultados\n",
    "acc_greedy = tree.evaluate_model(X_val, y_val, task='classification', metrics=['accuracy'])['accuracy']\n",
    "n_greedy = count_nodes(tree)\n",
    "\n",
    "start = time.time()\n",
    "_ = tree.predict(X_val, task='classification')\n",
    "t_greedy = time.time() - start\n",
    "\n",
    "profundidad_media_greedy = np.mean([path_length(x, tree) for x in X_scaled])\n",
    "\n",
    "print(f\"Resultados tras poda greedy:\")\n",
    "print(f\"   - Accuracy: {acc_greedy:.4f}\")\n",
    "print(f\"   - Nodos: {n_greedy} (reducción total: {n_before - n_greedy} nodos)\")\n",
    "print(f\"   - Tiempo de inferencia: {t_greedy:.4f}s\")\n",
    "print(f\"   - Profundidad media: {profundidad_media_greedy:.2f}\")\n",
    "print(f\"   - Cambio en accuracy: {acc_greedy - acc_before:+.4f}\")\n",
    "\n",
    "# Matriz de confusión\n",
    "print_confusion('Matriz de confusión DESPUÉS de poda greedy:', tree, X_val, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Comparación Visual de Resultados\n",
    "\n",
    "Visualizamos las mejoras obtenidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para visualización\n",
    "etapas = ['Original', 'Post-poda', 'Greedy']\n",
    "accuracies = [acc_before, acc_post, acc_greedy]\n",
    "nodos = [n_before, n_post, n_greedy]\n",
    "tiempos = [t_before, t_post, t_greedy]\n",
    "profundidades = [profundidad_media_before, profundidad_media_post, profundidad_media_greedy]\n",
    "\n",
    "# Crear figura con subplots\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Accuracy\n",
    "ax1.bar(etapas, accuracies, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax1.set_title('Accuracy por Etapa', fontsize=14, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_ylim(0.7, 0.8)\n",
    "for i, v in enumerate(accuracies):\n",
    "    ax1.text(i, v + 0.005, f'{v:.4f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. Número de nodos\n",
    "ax2.bar(etapas, nodos, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax2.set_title('Número de Nodos por Etapa', fontsize=14, fontweight='bold')\n",
    "ax2.set_ylabel('Nodos')\n",
    "for i, v in enumerate(nodos):\n",
    "    ax2.text(i, v + 10, f'{v}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Tiempo de inferencia\n",
    "ax3.bar(etapas, tiempos, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax3.set_title('Tiempo de Inferencia por Etapa', fontsize=14, fontweight='bold')\n",
    "ax3.set_ylabel('Tiempo (s)')\n",
    "for i, v in enumerate(tiempos):\n",
    "    ax3.text(i, v + 0.01, f'{v:.4f}s', ha='center', fontweight='bold')\n",
    "\n",
    "# 4. Profundidad media\n",
    "ax4.bar(etapas, profundidades, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "ax4.set_title('Profundidad Media por Etapa', fontsize=14, fontweight='bold')\n",
    "ax4.set_ylabel('Profundidad')\n",
    "for i, v in enumerate(profundidades):\n",
    "    ax4.text(i, v + 0.2, f'{v:.2f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Tabla resumen\n",
    "print(\"RESUMEN COMPARATIVO:\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'Etapa':<12} {'Accuracy':<10} {'Nodos':<8} {'Tiempo':<10} {'Profundidad':<12}\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Original':<12} {acc_before:<10.4f} {n_before:<8} {t_before:<10.4f} {profundidad_media_before:<12.2f}\")\n",
    "print(f\"{'Post-poda':<12} {acc_post:<10.4f} {n_post:<8} {t_post:<10.4f} {profundidad_media_post:<12.2f}\")\n",
    "print(f\"{'Greedy':<12} {acc_greedy:<10.4f} {n_greedy:<8} {t_greedy:<10.4f} {profundidad_media_greedy:<12.2f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Comparación de Velocidades\n",
    "\n",
    "Comparamos la velocidad de inferencia entre los métodos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Comparando velocidades de inferencia...\")\n",
    "\n",
    "# Función para medir tiempo de inferencia\n",
    "def infer_time(tree, X, method_name):\n",
    "    start = time.time()\n",
    "    _ = tree.predict(X, task='classification')\n",
    "    tiempo = time.time() - start\n",
    "    print(f\"   - {method_name}: {tiempo:.4f}s\")\n",
    "    return tiempo\n",
    "\n",
    "# Medir tiempos\n",
    "print(\"Tiempos de inferencia en conjunto de validación:\")\n",
    "t_original = infer_time(tree, X_val, \"Árbol Original\")\n",
    "t_post = infer_time(tree, X_val, \"Post-poda\")\n",
    "t_greedy = infer_time(tree, X_val, \"Greedy\")\n",
    "\n",
    "# Comparación visual de velocidades\n",
    "metodos = ['Original', 'Post-poda', 'Greedy']\n",
    "tiempos = [t_original, t_post, t_greedy]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metodos, tiempos, color=['#FF6B6B', '#4ECDC4', '#45B7D1'])\n",
    "plt.title('Comparación de Velocidades de Inferencia', fontsize=16, fontweight='bold')\n",
    "plt.ylabel('Tiempo (segundos)')\n",
    "plt.xlabel('Método')\n",
    "\n",
    "# Añadir valores en las barras\n",
    "for bar, tiempo in zip(bars, tiempos):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.001, \n",
    "             f'{tiempo:.4f}s', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nRESULTADO: La poda greedy es {t_original/t_greedy:.1f}x más rápida que el árbol original!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Conclusiones\n",
    "\n",
    "### Resumen de Mejoras Obtenidas:\n",
    "\n",
    "1. **Post-poda clásica**:\n",
    "   - Mantiene o mejora la precisión\n",
    "   - Reduce significativamente el número de nodos\n",
    "   - Conservadora y robusta\n",
    "\n",
    "2. **Poda greedy**:\n",
    "   - Reducción adicional de nodos\n",
    "   - Control de pérdida de precisión\n",
    "   - Mejora en tiempos de inferencia\n",
    "\n",
    "### Comparación de Métodos:\n",
    "\n",
    "| Aspecto | Post-Poda Clásica | Poda Greedy |\n",
    "|---------|-------------------|-------------|\n",
    "| **Conservadurismo** | Alta | Baja |\n",
    "| **Reducción de nodos** | Moderada | Alta |\n",
    "| **Mantenimiento de precisión** | Excelente | Variable |\n",
    "| **Facilidad de uso** | Alta | Media |\n",
    "| **Ajuste de parámetros** | Mínimo | Requerido |\n",
    "| **Robustez** | Alta | Media |\n",
    "\n",
    "### Recomendaciones:\n",
    "\n",
    "- **Para producción**: Usar post-poda clásica\n",
    "- **Para máxima velocidad**: Usar poda greedy\n",
    "- **Para interpretabilidad**: Usar solo post-poda clásica\n",
    "\n",
    "### Próximos Pasos:\n",
    "\n",
    "1. Probar con diferentes datasets\n",
    "2. Ajustar parámetros de tolerancia\n",
    "3. Implementar validación cruzada\n",
    "4. Comparar con otros métodos de optimización"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
} 